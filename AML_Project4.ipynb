{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNgEhDwnr7pGfYq0SE+HRzZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SimoneBorella/semantic-segmentation-domain-adaptation/blob/main/AML_Project4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gS8AxnMLzxaG"
      },
      "outputs": [],
      "source": [
        "#!pip3 install torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install 'tqdm'\n",
        "!pip install thop\n",
        "!pip install albumentations"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GwBrzPq007TP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "import albumentations as A\n",
        "import numpy as np\n",
        "import time\n",
        "from thop import profile\n",
        "import sys\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "vtiy-76X2Owu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Gabrysse/MLDL2024_project1.git\n",
        "\n",
        "sys.path.append('/content/MLDL2024_project1')\n",
        "print(os.path.exists('/content/MLDL2024_project1'))\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "r3iVgIks09Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/MLDL2024_project1/models/deeplabv2/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_RqoZCAO-lZ",
        "outputId": "f370d2b4-4532-4683-d2a6-e8c9ad2046af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deeplabv2.py  __pycache__\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the necessary directories if they do not exist\n",
        "os.makedirs('/content/dataset', exist_ok=True)\n",
        "\n",
        "\n",
        "# checks if datasets already exist before downloading\n",
        "if not os.path.exists('/content/dataset/Train.zip'):\n",
        "    !wget https://zenodo.org/records/5706578/files/Train.zip -P /content/dataset\n",
        "if not os.path.exists('/content/dataset/Val.zip'):\n",
        "    !wget https://zenodo.org/records/5706578/files/Val.zip -P /content/dataset\n",
        "\n",
        "# unzip the downloaded files if they have not been extracted\n",
        "if not os.path.exists('/content/dataset/Train'):\n",
        "    !unzip -q /content/dataset/Train.zip -d /content/dataset\n",
        "if not os.path.exists('/content/dataset/Val'):\n",
        "    !unzip -q /content/dataset/Val.zip -d /content/dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3fsOUnUQgnL",
        "outputId": "64043747-372b-49b6-a4c7-c5fb5324a884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-27 17:11:02--  https://zenodo.org/records/5706578/files/Train.zip\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.48.194, 188.185.43.25, 188.185.45.92, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.48.194|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4021669263 (3.7G) [application/octet-stream]\n",
            "Saving to: ‘/content/dataset/Train.zip’\n",
            "\n",
            "Train.zip           100%[===================>]   3.75G  12.7MB/s    in 5m 10s  \n",
            "\n",
            "2024-12-27 17:16:13 (12.4 MB/s) - ‘/content/dataset/Train.zip’ saved [4021669263/4021669263]\n",
            "\n",
            "--2024-12-27 17:16:13--  https://zenodo.org/records/5706578/files/Val.zip\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.48.194, 188.185.43.25, 188.185.45.92, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.48.194|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2425958254 (2.3G) [application/octet-stream]\n",
            "Saving to: ‘/content/dataset/Val.zip’\n",
            "\n",
            "Val.zip             100%[===================>]   2.26G  12.5MB/s    in 3m 8s   \n",
            "\n",
            "2024-12-27 17:19:22 (12.3 MB/s) - ‘/content/dataset/Val.zip’ saved [2425958254/2425958254]\n",
            "\n",
            "--2024-12-27 17:20:22--  https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 18.65.3.71, 18.65.3.38, 18.65.3.37, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|18.65.3.71|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 178728960 (170M) [application/octet-stream]\n",
            "Saving to: ‘pretrained_models/DeepLab_resnet_pretrained_imagenet.pth’\n",
            "\n",
            "pretrained_models/D 100%[===================>] 170.45M   125MB/s    in 1.4s    \n",
            "\n",
            "2024-12-27 17:20:24 (125 MB/s) - ‘pretrained_models/DeepLab_resnet_pretrained_imagenet.pth’ saved [178728960/178728960]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the pretrtained models\n",
        "os.makedirs('pretrained_models', exist_ok=True)\n",
        "\n",
        "!wget -c --no-check-certificate \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" -O \"pretrained_models/DeepLab_resnet_pretrained_imagenet.pth\"\n",
        "\n",
        "print(os.path.exists('pretrained_models/DeepLab_resnet_pretrained_imagenet.pth'))"
      ],
      "metadata": {
        "id": "E2QhWbjDN78K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the contents\n",
        "!ls /content/dataset/Train\n",
        "!ls /content/dataset/Val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwr15c9kUSCv",
        "outputId": "fd3127a4-ff4a-48e7-c1cb-f2f9568dabce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rural  Urban\n",
            "Rural  Urban\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "class LoveDAUrbanDataset(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.images = []\n",
        "        self.masks = []\n",
        "\n",
        "        # Paths for urban training data\n",
        "        image_dir = os.path.join(root_dir, 'Train/Urban/images_png')\n",
        "        mask_dir = os.path.join(root_dir, 'Train/Urban/masks_png')\n",
        "\n",
        "        for filename in os.listdir(image_dir):\n",
        "            if filename.endswith('.png'):\n",
        "                image_path = os.path.join(image_dir, filename)\n",
        "                mask_path = os.path.join(mask_dir, filename)\n",
        "\n",
        "                self.images.append(image_path)\n",
        "                self.masks.append(mask_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # loading image and mask\n",
        "        image = torchvision.io.read_image(self.images[idx]).float() / 255.0  # Normalize to [0, 1]\n",
        "        mask = torchvision.io.read_image(self.masks[idx], mode=torchvision.io.ImageReadMode.GRAY)\n",
        "\n",
        "        # resizing to 512x512 (consistent dimensions for model input)\n",
        "        # image = torchvision.transforms.functional.resize(image, size=(512, 512))\n",
        "        # mask = torchvision.transforms.functional.resize(mask, size=(512, 512), interpolation=Image.NEAREST)\n",
        "\n",
        "        image = F.resize(image, size=(512, 512))\n",
        "        mask = F.resize(mask, size=(512, 512), interpolation=Image.NEAREST)\n",
        "\n",
        "        return image, mask.long()\n"
      ],
      "metadata": {
        "id": "_xDNxWTwZW5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data transforms\n",
        "\n",
        "# def create_transforms():\n",
        "#    return A.Compose([\n",
        "#        A.Resize(512, 512),\n",
        "#        A.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "#                   std=[0.229, 0.224, 0.225])\n",
        "#    ])\n",
        "\n",
        "def create_transforms(mean, std):\n",
        "    return A.Compose([\n",
        "        A.Resize(512, 512),\n",
        "        A.Normalize(mean=mean, std=std),\n",
        "    ])    # ... to be completed later..."
      ],
      "metadata": {
        "id": "uOdsDvftuOJF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating Mean IoU\n",
        "def calculate_miou(model, dataloader, num_classes=7, device='cuda'):\n",
        "    model.eval()\n",
        "    confusion_matrix = np.zeros((num_classes, num_classes))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in tqdm(dataloader, desc=\"Calculating mIoU\"):\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            preds = outputs.argmax(1)\n",
        "\n",
        "            # updating the confusion matrix\n",
        "            for t, p in zip(masks.view(-1), preds.view(-1)):\n",
        "                confusion_matrix[t.long(), p.long()] += 1\n",
        "\n",
        "    # calculating IoU for each class\n",
        "    iou_per_class = []\n",
        "    for i in range(num_classes):\n",
        "        true_positive = confusion_matrix[i, i]\n",
        "        false_positive = confusion_matrix[:, i].sum() - true_positive\n",
        "        false_negative = confusion_matrix[i, :].sum() - true_positive\n",
        "\n",
        "        iou = true_positive / (true_positive + false_positive + false_negative + 1e-10)\n",
        "        iou_per_class.append(iou)\n",
        "\n",
        "    return np.mean(iou_per_class)\n"
      ],
      "metadata": {
        "id": "K8l-HPSmuTBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# measuring latency\n",
        "def measure_latency(model, input_size=(1, 3, 512, 512), device='cuda'):\n",
        "    model.eval()\n",
        "    x = torch.randn(input_size).to(device)\n",
        "\n",
        "    # warm-up phase\n",
        "    for _ in range(10):\n",
        "        _ = model(x)\n",
        "\n",
        "    # measuring time\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(100):\n",
        "            _ = model(x)\n",
        "    end_time = time.time()\n",
        "\n",
        "    return (end_time - start_time) / 100  # Avg latency"
      ],
      "metadata": {
        "id": "4UGQCgmCueYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset and dataloader\n",
        "\n",
        "# transforms = create_transforms()\n",
        "dataset = LoveDAUrbanDataset('/content/dataset')\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=2, pin_memory=True)"
      ],
      "metadata": {
        "id": "EIhK3arvu4LY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing DeepLabV2 from the provided repository\n",
        "\n",
        "from models.deeplabv2.deeplabv2 import get_deeplab_v2\n",
        "print(\"DeepLabv2 model imported :D\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = get_deeplab_v2(\n",
        "    num_classes=7,\n",
        "    pretrain=True,\n",
        "    pretrain_model_path='pretrained_models/DeepLab_resnet_pretrained_imagenet.pth'\n",
        ")\n",
        "\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "IWlIO7S1ZltT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "m1qZInzSwvJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n",
        "!free -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mStEhPsa7TMF",
        "outputId": "d01615ae-7227-4330-f1ad-b042bcb42c93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Dec 27 17:51:39 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0              26W /  70W |    277MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n",
            "               total        used        free      shared  buff/cache   available\n",
            "Mem:            12Gi       2.8Gi       262Mi        11Mi       9.6Gi       9.6Gi\n",
            "Swap:             0B          0B          0B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "# Training loop - ... needs to be fixed ...\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for images, masks in tqdm(dataloader, desc=f\"Epoch {epoch+1}/20\"):\n",
        "        images = images.to(device)\n",
        "        masks = masks.squeeze().to(device)   # removing extra dimension\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass\n",
        "        with torch.cuda.amp.autocast(device_type='cuda'):\n",
        "            outputs = model(images)\n",
        "            # If the model returns a tuple, extract the primary output\n",
        "            if isinstance(outputs, tuple):\n",
        "                outputs = outputs[0]\n",
        "            loss = criterion(outputs, masks)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Forward pass\n",
        "        # outputs = model(images)\n",
        "        # loss = criterion(outputs, masks)\n",
        "\n",
        "        # Backward pass\n",
        "        # optimizer.zero_grad()\n",
        "        # loss.backward()\n",
        "        # optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Epoch [{epoch+1}/20], Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "ZDNwL7VsvFiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # calculating metrics\n",
        "print(\"\\nCalculating metrics...\")\n",
        "\n",
        "# 1. Mean IoU\n",
        "miou = calculate_miou(model, dataloader, device=device)\n",
        "print(f\"Mean IoU: {miou:.4f}\")\n",
        "\n",
        "# 2. Latency\n",
        "latency = measure_latency(model, device=device)\n",
        "print(f\"Latency: {latency*1000:.2f} ms\")\n",
        "\n",
        "# 3. FLOPs (floating point operations) and Parameters\n",
        "input_tensor = torch.randn(1, 3, 512, 512).to(device)\n",
        "flops, params = profile(model, inputs=(input_tensor,))\n",
        "print(f\"FLOPs: {flops/1e9:.2f}G\")\n",
        "print(f\"Parameters: {params/1e6:.2f}M\")\n",
        "\n",
        "# saving the results\n",
        "results = {\n",
        "    'miou': miou,\n",
        "    'latency': latency,\n",
        "    'flops': flops,\n",
        "    'params': params\n",
        "}"
      ],
      "metadata": {
        "id": "h7oYeIW_vjbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Save model\n",
        "# torch.save(model.state_dict(), 'deeplabv2_loveda.pth')\n",
        "# print(\"\\nTraining completed and model saved!\")\n",
        "\n",
        "return results"
      ],
      "metadata": {
        "id": "pUvbBzMFvtyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sdCcmf49vmwA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}